{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "518b7bbc",
      "metadata": {
        "id": "518b7bbc"
      },
      "source": [
        "# Representação Vetorial de Texto\n",
        "\n",
        "Modelos de aprendizado de máquina, incluindo redes neurais, não operam sobre texto puro. Eles requerem que a entrada de dados seja representada como vetores numéricos. O campo do Processamento de Linguagem Natural (PLN) desenvolveu diversas técnicas para realizar essa conversão, um processo conhecido como *feature extraction* ou vetorização. A qualidade dessa representação vetorial é fundamental para o desempenho do modelo final, pois ela deve capturar as propriedades sintáticas e semânticas da linguagem de forma que o modelo possa generalizar o conhecimento adquirido."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc5b8d18",
      "metadata": {
        "id": "dc5b8d18"
      },
      "source": [
        "# 1. Tokenização\n",
        "\n",
        "Antes que qualquer representação vetorial possa ser criada, o texto bruto deve ser segmentado em suas unidades constituintes. Esse processo é chamado de **tokenização**. As unidades resultantes são chamadas de **tokens**. A escolha da estratégia de tokenização é um passo fundamental que impacta todo o pipeline de PLN.\n",
        "\n",
        "Após a tokenização, o próximo passo é construir um **vocabulário**, que é o conjunto de todos os tokens únicos presentes em um corpus de texto. A finalidade do vocabulário é criar um mapeamento determinístico entre cada token e um índice numérico (um inteiro). Esse mapeamento é a base para todas as formas de representação vetorial."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "659592d3",
      "metadata": {
        "id": "659592d3"
      },
      "source": [
        "## Tokenização por Caractere\n",
        "\n",
        "A forma mais granular de tokenização é tratar cada caractere como um token individual.\n",
        "\n",
        "* **Vantagens**: O vocabulário é muito pequeno e controlado (letras, números, símbolos). O modelo nunca encontrará um token \"fora do vocabulário\" (out-of-vocabulary, OOV).\n",
        "* **Desvantagens**: A noção de palavra é perdida no nível de entrada, e o modelo precisa aprender a compor palavras a partir de caracteres. As sequências de entrada tornam-se muito longas, aumentando o custo computacional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be2bfc6f",
      "metadata": {
        "id": "be2bfc6f"
      },
      "outputs": [],
      "source": [
        "text_sample = \"IA Generativa.\"\n",
        "\n",
        "# Tokenização por caractere\n",
        "char_tokens = list(text_sample)\n",
        "char_vocab = sorted(list(set(char_tokens)))\n",
        "char_to_ix = {ch: i for i, ch in enumerate(char_vocab)}\n",
        "\n",
        "# Mapeando tokens para inteiros\n",
        "encoded_chars = [char_to_ix[ch] for ch in char_tokens]\n",
        "\n",
        "print(f\"Texto original: '{text_sample}'\")\n",
        "print(f\"Tokens de caractere: {char_tokens}\")\n",
        "print(f\"Vocabulário de caracteres: {char_vocab}\")\n",
        "print(f\"Sequência codificada: {encoded_chars}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88450da2",
      "metadata": {
        "id": "88450da2"
      },
      "source": [
        "## Tokenização por Palavra\n",
        "\n",
        "Esta é a abordagem mais intuitiva, onde o texto é dividido com base em espaços em branco e pontuação.\n",
        "\n",
        "* **Vantagens**: Preserva a unidade semântica da palavra. É computacionalmente eficiente.\n",
        "* **Desvantagens**: O tamanho do vocabulário pode se tornar extremamente grande. O modelo é incapaz de lidar com palavras que não viu durante o treinamento (problema OOV). Palavras com flexões morfológicas (e.g., \"aprende\", \"aprendendo\", \"aprendeu\") são tratadas como tokens distintos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c8a11d1",
      "metadata": {
        "id": "0c8a11d1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text_sample = \"IA Generativa é um campo fascinante, talvez o mais importante da inteligência artificial!\"\n",
        "\n",
        "# Regex para encontrar sequências de caracteres alfanuméricos (incluindo acentos) ou pontuações isoladas.\n",
        "word_tokens = re.findall(r'[\\w]+|[.,!?]', text_sample.lower())\n",
        "word_vocab = sorted(list(set(word_tokens)))\n",
        "word_to_ix_map = {word: i for i, word in enumerate(word_vocab)}\n",
        "\n",
        "print(f\"Texto original: '{text_sample}'\")\n",
        "print(f\"Tokens de palavra: {word_tokens}\")\n",
        "print(f\"Vocabulário de palavras: {word_vocab}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bbd1350",
      "metadata": {
        "id": "0bbd1350"
      },
      "source": [
        "## Tokenização por Subpalavra\n",
        "\n",
        "A tokenização por subpalavra busca um meio-termo, segmentando palavras raras em unidades menores e mais frequentes, enquanto mantém palavras comuns como tokens únicos.\n",
        "\n",
        "* **Vantagens**: Controla o tamanho do vocabulário, elimina o problema de OOV e pode capturar relações morfológicas (e.g., \"generativa\" e \"geração\" podem compartilhar o subtoken \"gera\").\n",
        "* **Desvantagens**: A implementação é mais complexa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a02c96",
      "metadata": {
        "id": "95a02c96"
      },
      "outputs": [],
      "source": [
        "# Vocabulário hipotético de subpalavras\n",
        "subword_vocab = {\"ia\", \"generativa\", \"aprend\", \"##izado\", \"##endo\", \"supervision\", \"##ado\"}\n",
        "\n",
        "# Palavra a ser tokenizada\n",
        "word = \"aprendizado\"\n",
        "\n",
        "# Lógica de tokenização simplificada\n",
        "tokens = []\n",
        "# Encontra a maior subpalavra do vocabulário que corresponde ao início da palavra\n",
        "if word.startswith(\"aprend\"):\n",
        "    tokens.append(\"aprend\")\n",
        "    remaining_part = word[len(\"aprend\"):] # \"izado\"\n",
        "    # Adiciona o prefixo '##' para buscar a continuação\n",
        "    if \"##\" + remaining_part in subword_vocab:\n",
        "        tokens.append(\"##\" + remaining_part)\n",
        "\n",
        "print(f\"A palavra '{word}' é tokenizada em: {tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b46082",
      "metadata": {
        "id": "25b46082"
      },
      "outputs": [],
      "source": [
        "# Palavra desconhecida\n",
        "word = \"supervisionado\"\n",
        "tokens = []\n",
        "if word.startswith(\"supervision\"):\n",
        "    tokens.append(\"supervision\")\n",
        "    remaining_part = word[len(\"supervision\"):]\n",
        "    if \"##\" + remaining_part in subword_vocab:\n",
        "        tokens.append(\"##\" + remaining_part)\n",
        "\n",
        "print(f\"A palavra '{word}' é tokenizada em: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a6044f",
      "metadata": {
        "id": "f4a6044f"
      },
      "source": [
        "# 2. One-Hot Encoding\n",
        "\n",
        "Com um vocabulário de tokens definido, a abordagem mais simples de vetorização é o One-Hot Encoding (OHE). Para um vocabulário de tamanho $V$, cada token $w$ com índice $i_w$ é representado por um vetor $v_w$ de dimensão $V$. Formalmente, a j-ésima componente do vetor $v_w$ é dada pela função delta de Kronecker:\n",
        "\n",
        "$$(v_w)_j = \\delta_{i_w, j} = \\begin{cases} 1 & \\text{se } j = i_w \\\\ 0 & \\text{se } j \\neq i_w \\end{cases}$$\n",
        "\n",
        "Por exemplo, considere o vocabulário `{\"o\": 0, \"gato\": 1, \"bebe\": 2}`.\n",
        "\n",
        "* O token \"gato\" tem índice 1. Seu vetor OHE é: `[0, 1, 0]`\n",
        "* O token \"o\" tem índice 0. Seu vetor OHE é: `[1, 0, 0]`\n",
        "\n",
        "A sequência \"o gato\" seria representada por uma lista de vetores: `[[1, 0, 0], [0, 1, 0]]`.\n",
        "\n",
        "A principal limitação é a **ortogonalidade** ($v_i^T v_j = 0$ para $i \\neq j$), que impede o modelo de capturar qualquer noção de similaridade entre os tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e8b733",
      "metadata": {
        "id": "25e8b733"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Vocabulário e mapeamento\n",
        "vocab = {\"o\": 0, \"gato\": 1, \"bebe\": 2}\n",
        "vocab_size = len(vocab)\n",
        "word_to_ix = vocab\n",
        "\n",
        "def one_hot_encode(word, word_to_ix):\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    if word in word_to_ix:\n",
        "        vec[word_to_ix[word]] = 1\n",
        "    return vec\n",
        "\n",
        "# Exemplo\n",
        "word_ohe = one_hot_encode(\"gato\", word_to_ix)\n",
        "print(f\"Vocabulário: {vocab}\")\n",
        "print(f\"Representação One-Hot: {word_ohe}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c9b97cb",
      "metadata": {
        "id": "7c9b97cb"
      },
      "source": [
        "# 3. Representações Densas: Word Embeddings\n",
        "\n",
        "As representações esparsas como o One-Hot Encoding apresentam duas desvantagens críticas:\n",
        "1.  **A Maldição da Dimensionalidade**: Para um vocabulário realista (dezenas de milhares de palavras), os vetores se tornam extremamente grandes e esparsos, tornando o processamento computacionalmente caro e exigindo uma quantidade massiva de dados para que um modelo possa aprender padrões significativos.\n",
        "2.  **Ausência de Relação Semântica**: A ortogonalidade dos vetores implica que não há uma noção de similaridade. O modelo não consegue inferir que \"cão\" e \"gato\" são mais parecidos entre si do que com a palavra \"algoritmo\".\n",
        "\n",
        "Para superar isso, foram desenvolvidas as **representações densas**, conhecidas como **word embeddings**. A ideia é representar cada palavra por um vetor de baixa dimensão (tipicamente de 50 a 300 dimensões) com valores de ponto flutuante (números reais). Em vez de cada dimensão corresponder a uma palavra específica, as dimensões representam *features latentes* ou atributos do significado da palavra, que são aprendidos automaticamente a partir dos dados.\n",
        "\n",
        "O princípio fundamental por trás do aprendizado desses embeddings é a **Hipótese Distribucional**: \"uma palavra é caracterizada pela companhia que mantém\" (Firth, 1957). Isso significa que palavras que aparecem em contextos textuais similares tendem a ter significados similares. O objetivo, portanto, é aprender uma função de mapeamento $E: w \\rightarrow \\mathbb{R}^d$ (onde $d \\ll |V|$) tal que, se duas palavras $w_1$ e $w_2$ são semanticamente similares, seus vetores $E(w_1)$ e $E(w_2)$ estarão próximos no espaço vetorial."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0640e12",
      "metadata": {
        "id": "e0640e12"
      },
      "source": [
        "## O Conceito de Espaço Vetorial Semântico\n",
        "\n",
        "Esse processo de aprendizado cria um **espaço vetorial semântico**, onde a geometria (distância e direção) entre os vetores captura relações de significado. Por exemplo, a relação vetorial entre \"rei\" e \"rainha\" pode ser similar à relação entre \"homem\" e \"mulher\", permitindo raciocínios por analogia como:\n",
        "\n",
        "$$ \\vec{v}_{\\text{rei}} - \\vec{v}_{\\text{homem}} + \\vec{v}_{\\text{mulher}} \\approx \\vec{v}_{\\text{rainha}} $$\n",
        "\n",
        "Modelos como o Word2Vec são projetados para construir esse espaço a partir de grandes volumes de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8e4660",
      "metadata": {
        "id": "dd8e4660"
      },
      "source": [
        "## Word2Vec e a Arquitetura Skip-gram\n",
        "\n",
        "Word2Vec é uma família de modelos que aprende esses embeddings de forma eficiente. Vamos focar na sua arquitetura mais popular, o **Skip-gram**.\n",
        "\n",
        "### Objetivo e Estrutura do Skip-gram\n",
        "\n",
        "O objetivo do Skip-gram é, dada uma palavra central $w_c$, prever as palavras que aparecem em sua janela de contexto, $w_o$. O modelo é uma rede neural simples com uma única camada oculta.\n",
        "\n",
        "1.  **Entrada**: A palavra central $w_c$ é representada como um vetor one-hot de tamanho $|V|$.\n",
        "2.  **Matriz de Embedding ($W$)**: Uma matriz de pesos de dimensão $|V| \\times d$. Multiplicar o vetor one-hot da palavra de entrada por esta matriz é equivalente a selecionar a linha correspondente ao índice da palavra, obtendo seu vetor de embedding $v_{w_c}$ (a camada oculta).\n",
        "3.  **Matriz de Saída ($W'$)**: Uma segunda matriz de pesos, de dimensão $d \\times |V|$.\n",
        "4.  **Cálculo dos Scores**: O embedding da palavra central $v_{w_c}$ é multiplicado pela matriz de saída $W'$ para produzir um vetor de scores $u$ de dimensão $|V|$. Cada elemento $u_j$ deste vetor é o produto escalar $u_j = v'_{w_j} \\cdot v_{w_c}$, onde $v'_{w_j}$ é o \"embedding de saída\" da j-ésima palavra do vocabulário. Este score mede a similaridade entre a palavra central $w_c$ e cada outra palavra $w_j$ como um potencial contexto.\n",
        "5.  **Probabilidades de Saída**: Os scores são convertidos em probabilidades usando a função softmax. A probabilidade de uma palavra $w_o$ ser uma palavra de contexto de $w_c$ é dada por:\n",
        "\n",
        "$$P(w_o | w_c) = \\frac{\\exp(v'_{w_o} \\cdot v_{w_c})}{\\sum_{j=1}^{|V|} \\exp(v'_{w_j} \\cdot v_{w_c})}$$\n",
        "\n",
        "### Função Objetivo (Loss)\n",
        "\n",
        "O objetivo do treinamento é ajustar os pesos das matrizes $W$ (os embeddings de entrada) e $W'$ (os embeddings de saída) para maximizar a probabilidade de prever corretamente as palavras de contexto reais. Isso é feito minimizando a função de perda (loss), que é o log-likelihood negativo médio sobre todo o corpus. Para uma única palavra central $w_t$, a perda para seu contexto $C_t$ é:\n",
        "\n",
        "$$L = - \\sum_{w_j \\in C_t} \\log P(w_j | w_t)$$\n",
        "\n",
        "Ao final do treinamento, a matriz $W$ contém os vetores de embedding densos e semanticamente ricos que procuramos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3b439c",
      "metadata": {
        "id": "5f3b439c"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8524edeb",
      "metadata": {
        "id": "8524edeb"
      },
      "outputs": [],
      "source": [
        "# Corpus temático sobre IA\n",
        "corpus_text = \"\"\"\n",
        "o modelo de aprendizado supervisionado utiliza dados rotulados.\n",
        "um algoritmo de aprendizado profundo treina uma rede neural artificial.\n",
        "a rede neural aprende com os dados de entrada.\n",
        "o modelo de clusterizacao é um algoritmo de aprendizado nao supervisionado.\n",
        "o aprendizado por reforco otimiza uma recompensa.\n",
        "o modelo generativo cria novos dados.\n",
        "um bom algoritmo precisa de bons dados para o treinamento.\n",
        "a rede convolucional é um tipo de rede neural para imagens.\n",
        "o treinamento de um modelo profundo exige muito poder computacional.\n",
        "\"\"\"\n",
        "\n",
        "# Pré-processamento e criação do vocabulário\n",
        "corpus_words = re.findall(r'[\\w]+', corpus_text.lower())\n",
        "vocab = sorted(list(set(corpus_words)))\n",
        "vocab_size = len(vocab)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b3c444",
      "metadata": {
        "id": "b6b3c444"
      },
      "outputs": [],
      "source": [
        "# Geração de pares de treinamento\n",
        "window_size = 2\n",
        "skipgram_data = []\n",
        "for i in range(window_size, len(corpus_words) - window_size):\n",
        "    center_word = corpus_words[i]\n",
        "    context_indices = list(range(i - window_size, i)) + list(range(i + 1, i + window_size + 1))\n",
        "    for j in context_indices:\n",
        "        skipgram_data.append((center_word, corpus_words[j]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e8fca6",
      "metadata": {
        "id": "90e8fca6"
      },
      "outputs": [],
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "    def forward(self, center_word_idx):\n",
        "        embeds = self.embeddings(center_word_idx)\n",
        "        out = self.linear(embeds)\n",
        "        return nn.functional.log_softmax(out, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea4f8d2e",
      "metadata": {
        "id": "ea4f8d2e"
      },
      "outputs": [],
      "source": [
        "# Treinamento do modelo\n",
        "embedding_dim = 8\n",
        "model = SkipGramModel(vocab_size, embedding_dim)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(250):\n",
        "    for center_word_str, context_word_str in skipgram_data:\n",
        "        center_word_idx = torch.tensor([word_to_ix[center_word_str]], dtype=torch.long)\n",
        "        context_word_idx = torch.tensor([word_to_ix[context_word_str]], dtype=torch.long)\n",
        "        model.zero_grad()\n",
        "        log_probs = model(center_word_idx)\n",
        "        loss = loss_function(log_probs, context_word_idx)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7133ad",
      "metadata": {
        "id": "0c7133ad"
      },
      "outputs": [],
      "source": [
        "# Visualização\n",
        "learned_embeddings = model.embeddings.weight.data.numpy()\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=min(10, vocab_size - 1))\n",
        "embeddings_2d = tsne.fit_transform(learned_embeddings)\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "for i, word in enumerate(vocab):\n",
        "    x, y = embeddings_2d[i, :]\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
        "plt.title(\"Visualização de Embeddings\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc419ec7",
      "metadata": {
        "id": "cc419ec7"
      },
      "source": [
        "# 4. Utilizando Modelos Word2Vec Pré-treinados\n",
        "\n",
        "Treinar embeddings do zero é custoso e exige um corpus massivo. Na prática, é comum utilizar modelos pré-treinados em bilhões de palavras (e.g., Wikipedia, Google News). Esses modelos fornecem representações semânticas robustas e de alta qualidade que podem ser usadas diretamente em tarefas de PLN.\n",
        "\n",
        "A biblioteca `gensim` facilita o download e o uso desses modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1107751a",
      "metadata": {
        "id": "1107751a"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "# Carregando um modelo pré-treinado (GloVe treinado na Wikipedia, 100 dimensões)\n",
        "# Modelos disponíveis: print(list(api.info()['models'].keys()))\n",
        "print(\"Carregando modelo GloVe\")\n",
        "glove_model = api.load('glove-wiki-gigaword-100')\n",
        "print(\"Modelo carregado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f0c125",
      "metadata": {
        "id": "61f0c125"
      },
      "source": [
        "### Explorando o Modelo Pré-treinado\n",
        "\n",
        "Com o modelo carregado, podemos realizar operações semânticas, como encontrar as palavras mais similares a um dado termo ou resolver analogias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f10ff5d",
      "metadata": {
        "id": "1f10ff5d"
      },
      "outputs": [],
      "source": [
        "print(\"Palavras mais similares a 'king':\")\n",
        "print(glove_model.most_similar('king'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d929940",
      "metadata": {
        "id": "3d929940"
      },
      "outputs": [],
      "source": [
        "# Resolvendo a analogia: king - man + woman = queen\n",
        "print(\"Resolvendo a analogia 'king - man + woman':\")\n",
        "print(glove_model.most_similar(positive=['woman', 'king'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "951a5dda",
      "metadata": {
        "id": "951a5dda"
      },
      "source": [
        "### Visualizando Embeddings Pré-treinados\n",
        "\n",
        "Podemos extrair os vetores de um conjunto de palavras e visualizá-los com t-SNE para confirmar que o espaço semântico é coerente. Note como palavras de categorias similares (países, animais, verbos) se agrupam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd338400",
      "metadata": {
        "id": "fd338400"
      },
      "outputs": [],
      "source": [
        "# Selecionando um conjunto de palavras para visualização\n",
        "words_to_plot = [\n",
        "    'king', 'queen', 'prince', 'princess', # Realeza\n",
        "    'dog', 'cat', 'lion', 'tiger',       # Animais\n",
        "    'brazil', 'france', 'japan', 'egypt',  # Países\n",
        "    'run', 'walk', 'swim', 'fly'         # Verbos\n",
        "]\n",
        "\n",
        "# Filtrando palavras que existem no vocabulário do modelo\n",
        "valid_words = [word for word in words_to_plot if word in glove_model]\n",
        "word_vectors = np.array([glove_model[word] for word in valid_words])\n",
        "\n",
        "# Redução de dimensionalidade com t-SNE\n",
        "tsne_plot = TSNE(n_components=2, random_state=42, perplexity=min(10, len(valid_words) - 1))\n",
        "embeddings_2d_plot = tsne_plot.fit_transform(word_vectors)\n",
        "\n",
        "# Plotando\n",
        "plt.figure(figsize=(14, 10))\n",
        "for i, word in enumerate(valid_words):\n",
        "    x, y = embeddings_2d_plot[i, :]\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
        "\n",
        "plt.title(\"Visualização de Embeddings Pré-treinados (GloVe)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a95df2",
      "metadata": {
        "id": "a8a95df2"
      },
      "source": [
        "# Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4540954",
      "metadata": {
        "id": "e4540954"
      },
      "source": [
        "## Exercício 1: Construindo um Tokenizador de Palavras\n",
        "\n",
        "Crie uma função Python chamada `tokenizador(texto)` que receba uma string como entrada e retorne uma lista de tokens.\n",
        "\n",
        "Sua função deve atender aos seguintes critérios:\n",
        "1.  Converter todo o texto para letras minúsculas.\n",
        "2.  Separar pontuações comuns (como `.`, `,`, `!`, `?`) das palavras. Por exemplo, \"Olá!\" deve se tornar `['olá', '!']`.\n",
        "3.  Dividir o texto em tokens com base nos espaços em branco."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenizador(texto):\n",
        "    # Converter o texto para letras minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Separar palavras (sequências alfanuméricas, incluindo acentos) e pontuações comuns\n",
        "    tokens = re.findall(r'[\\w]+|[.,!?]', texto)\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "svi8Lv_ip3XB"
      },
      "id": "svi8Lv_ip3XB",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d95f998",
      "metadata": {
        "id": "4d95f998"
      },
      "source": [
        "## Exercício 2: Criando Representações One-Hot\n",
        "\n",
        "Utilizando a função `tokenizador` do exercício anterior, crie um pipeline para gerar vetores One-Hot.\n",
        "\n",
        "Você precisará de duas funções:\n",
        "1.  `construir_vocabulario(texto, tokenizador)`: Recebe um texto e sua função de tokenização, e retorna um dicionário que mapeia cada token único a um índice (ex: `{'mundo': 0, 'olá': 1, ...}`).\n",
        "2.  `codificar_one_hot(palavra, vocabulario)`: Recebe uma palavra e o mapa de vocabulário, e retorna seu vetor One-Hot correspondente como um `torch.Tensor`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def construir_vocabulario(texto, tokenizador):\n",
        "    # Tokenizar o texto usando a função tokenizador\n",
        "    tokens = tokenizador(texto)\n",
        "\n",
        "    # Criar um conjunto de tokens únicos e ordená-los\n",
        "    vocab = sorted(set(tokens))\n",
        "\n",
        "    # Criar um dicionário que mapeia cada token a um índice\n",
        "    vocabulario = {token: idx for idx, token in enumerate(vocab)}\n",
        "\n",
        "    return vocabulario\n",
        "\n",
        "def codificar_one_hot(palavra, vocabulario):\n",
        "    # Obter o tamanho do vocabulário\n",
        "    vocab_size = len(vocabulario)\n",
        "\n",
        "    # Criar um vetor one-hot de zeros\n",
        "    one_hot = torch.zeros(vocab_size)\n",
        "\n",
        "    # Se a palavra está no vocabulário, marcar a posição correspondente com 1\n",
        "    if palavra in vocabulario:\n",
        "        one_hot[vocabulario[palavra]] = 1\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "IUHJtWWCqCk2"
      },
      "id": "IUHJtWWCqCk2",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4b4aadfc",
      "metadata": {
        "id": "4b4aadfc"
      },
      "source": [
        "## Exercício 3: Analogias de Capitais com Embeddings Pré-treinados\n",
        "\n",
        "Embeddings de palavras capturam relações semânticas. Uma das analogias mais famosas é a de capitais: \"Atenas está para a Grécia assim como Oslo está para a Noruega\". Matematicamente, isso se traduz em:\n",
        "\n",
        "$$ \\vec{v}_{\\text{Atenas}} - \\vec{v}_{\\text{Grécia}} + \\vec{v}_{\\text{Noruega}} \\approx \\vec{v}_{\\text{Oslo}} $$\n",
        "\n",
        "Use o modelo `glove_model` carregado anteriormente para resolver a seguinte analogia: **\"Alemanha está para Berlim assim como Brasil está para ?\"**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "SyzVJhLAqVNa"
      },
      "id": "SyzVJhLAqVNa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Carregar o modelo GloVe pré-treinado\n",
        "print(\"Carregando modelo GloVe...\")\n",
        "glove_model = api.load('glove-wiki-gigaword-100')\n",
        "print(\"Modelo carregado.\")\n",
        "\n",
        "# Resolver a analogia: Alemanha está para Berlim assim como Brasil está para ?\n",
        "print(\"Resolvendo a analogia 'Alemanha está para Berlim assim como Brasil está para ?':\")\n",
        "result = glove_model.most_similar(positive=['brazil', 'berlin'], negative=['germany'], topn=1)\n",
        "print(f\"A palavra mais próxima é: {result[0][0]} (similaridade: {result[0][1]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZAOn3yxqMlW",
        "outputId": "4bc895f3-eb25-4dc6-fa1d-cdd4e2af23fb"
      },
      "id": "lZAOn3yxqMlW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando modelo GloVe...\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Modelo carregado.\n",
            "Resolvendo a analogia 'Alemanha está para Berlim assim como Brasil está para ?':\n",
            "A palavra mais próxima é: janeiro (similaridade: 0.7682)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd1930a",
      "metadata": {
        "id": "2cd1930a"
      },
      "source": [
        "## Exercício 4: O Desafio da Polissemia\n",
        "\n",
        "Um dos problemas de modelos de embedding estáticos como Word2Vec e GloVe é a **polissemia**: uma única palavra pode ter múltiplos significados, mas recebe apenas um único vetor.\n",
        "\n",
        "O seu desafio é investigar a palavra \"bank\", que em inglês pode significar uma instituição financeira ou a margem de um rio.\n",
        "\n",
        "1.  Use o `glove_model` para encontrar os 10 vizinhos mais próximos da palavra \"bank\".\n",
        "2.  Analise a lista de vizinhos. Eles se referem ao contexto financeiro, ao geográfico, ou a uma mistura de ambos?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o modelo GloVe pré-treinado\n",
        "print(\"Carregando modelo GloVe...\")\n",
        "glove_model = api.load('glove-wiki-gigaword-100')\n",
        "print(\"Modelo carregado.\")\n",
        "\n",
        "# Encontrar os 10 vizinhos mais próximos da palavra \"bank\"\n",
        "print(\"Palavras mais similares a 'bank':\")\n",
        "vizinhos = glove_model.most_similar('bank', topn=10)\n",
        "for palavra, similaridade in vizinhos:\n",
        "    print(f\"{palavra}: {similaridade:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa1vr9zPq5vj",
        "outputId": "6806946e-5d61-41e3-bd26-40f0e9b9b3ec"
      },
      "id": "sa1vr9zPq5vj",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando modelo GloVe...\n",
            "Modelo carregado.\n",
            "Palavras mais similares a 'bank':\n",
            "banks: 0.8057\n",
            "banking: 0.7531\n",
            "credit: 0.7038\n",
            "investment: 0.6940\n",
            "financial: 0.6777\n",
            "securities: 0.6688\n",
            "lending: 0.6645\n",
            "funds: 0.6485\n",
            "ubs: 0.6483\n",
            "finance: 0.6462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lista de vizinhos se referem mais ao contexto financeiro"
      ],
      "metadata": {
        "id": "C9TeMwXvsIlR"
      },
      "id": "C9TeMwXvsIlR"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}